{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8675, 1)\n",
      "                                                  posts\n",
      "type                                                   \n",
      "INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
      "ENTP  'I'm finding the lack of me in these posts ver...\n",
      "INTP  'Good one  _____   https://www.youtube.com/wat...\n",
      "INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
      "ENTJ  'You're fired.|||That's another silly misconce...\n"
     ]
    }
   ],
   "source": [
    "text=pd.read_csv(\"mbti_1.csv\" ,index_col='type')\n",
    "print(text.shape)\n",
    "print(text[0:5])\n",
    "#print(text.iloc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# One hot encode labels\n",
    "labels=text.index.tolist()\n",
    "encoder=LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)\n",
    "labels=encoder.fit_transform(labels)\n",
    "labels=np.array(labels)\n",
    "print(labels[50:55])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mbti_dict={0:'ENFJ',1:'ENFP',2:'ENTJ',3:'ENTP',4:'ESFJ',5:'ESFP',6:'ESTJ',7:'ESTP',8:'INFJ',9:'INFP',10:'INTJ',11:'INTP',12:'ISFJ',13:'ISFP',14:'ISFP',15:'ISTP'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean data..\n",
    "def post_cleaner(post):\n",
    "    \"\"\"cleans individual posts`.\n",
    "    Args:\n",
    "        post-string\n",
    "    Returns:\n",
    "         cleaned up post`.\n",
    "    \"\"\"\n",
    "    # Covert all uppercase characters to lower case\n",
    "    post = post.lower() \n",
    "    \n",
    "    # Remove |||\n",
    "    post=post.replace('|||',\"\") \n",
    "\n",
    "    # Remove URLs, links etc\n",
    "    post = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', '', post, flags=re.MULTILINE) \n",
    "    # This would have removed most of the links but probably not all \n",
    "\n",
    "    # Remove puntuations \n",
    "    puncs1=['@','#','$','%','^','&','*','(',')','-','_','+','=','{','}','[',']','|','\\\\','\"',\"'\",';',':','<','>','/']\n",
    "    for punc in puncs1:\n",
    "        post=post.replace(punc,'') \n",
    "\n",
    "    puncs2=[',','.','?','!','\\n']\n",
    "    for punc in puncs2:\n",
    "        post=post.replace(punc,' ') \n",
    "    # Remove extra white spaces\n",
    "    post=re.sub( '\\s+', ' ', post ).strip()\n",
    "    return post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts=text.posts.tolist()\n",
    "posts=[post_cleaner(post) for post in posts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172989\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_count=Counter()\n",
    "for post in posts:\n",
    "    word_count.update(post.split(\" \"))\n",
    "    \n",
    "vocab_len=len(word_count)\n",
    "print(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = sorted(word_count, key=word_count.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "posts_ints=[]\n",
    "\n",
    "for post in posts:\n",
    "    posts_ints.append([vocab_to_int[word] for word in post.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 141, 1287, 91380, 22, 703, 1852, 2075, 139374, 89]\n",
      "566\n"
     ]
    }
   ],
   "source": [
    "print(posts_ints[0][:10])\n",
    "print(len(posts_ints[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5321.75576391\n"
     ]
    }
   ],
   "source": [
    "posts_lens = Counter([len(x) for x in posts])\n",
    "print(np.mean(posts_lens.keys())- (0.5*np.std(posts_lens.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_len = 1000\n",
    "features=np.zeros((len(posts_ints),seq_len),dtype=int)\n",
    "for i, row in enumerate(posts_ints):\n",
    "    features[i, -len(row):] = np.array(row)[:seq_len]\n",
    "#print(features[1555])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_frac = 0.8\n",
    "\n",
    "num_ele=int(split_frac*len(features))\n",
    "rem_ele=len(features)-num_ele\n",
    "train_x, val_x = features[:num_ele],features[num_ele: num_ele + int(rem_ele/2)]\n",
    "train_y, val_y = labels[:num_ele],labels[num_ele:num_ele + int(rem_ele/2)]\n",
    "\n",
    "test_x =features[num_ele + int(rem_ele/2):]\n",
    "test_y = labels[num_ele + int(rem_ele/2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_size = 256\n",
    "lstm_layers = 1\n",
    "batch_size = 256\n",
    "learning_rate = 0.01\n",
    "embed_dim=250\n",
    "n_words = len(vocab_to_int) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "int_to_vocab = {ii: word for word,ii in vocab_to_int.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model[int_to_vocab[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116753"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_emb = []\n",
    "unk=[]\n",
    "for k in range(1,len(int_to_vocab)):\n",
    "    try:\n",
    "        wv_emb.append(model[int_to_vocab[k]])\n",
    "    except:\n",
    "        unk.append(int_to_vocab[k])\n",
    "len(unk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hisher',\n",
       " 'winki',\n",
       " 'thatd',\n",
       " 'keirsey',\n",
       " 'enxp',\n",
       " 'niti',\n",
       " '65',\n",
       " '36',\n",
       " 'favourites',\n",
       " 'travelling']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unk[160:170]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "    labels_ = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    embedding= tf.Variable(tf.random_uniform(shape=(n_words,embed_dim),minval=-1,maxval=1))\n",
    "    embed=tf.nn.embedding_lookup(embedding,input_data)\n",
    "    #print(embed.shape)\n",
    "    \n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm,output_keep_prob=keep_prob)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop]* lstm_layers)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    outputs,final_state=tf.nn.dynamic_rnn(cell,embed,dtype=tf.float32 )\n",
    "    pre = tf.layers.dense(outputs[:,-1], 16, activation=tf.nn.relu)\n",
    "    predictions=tf.layers.dense(pre, 16, activation=tf.nn.softmax)\n",
    "    \n",
    "    cost = tf.losses.mean_squared_error(labels_, predictions)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    \n",
    "    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size=100):\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 0/3', 'Iteration: 5', 'Train loss: 0.058')\n",
      "('Epoch: 0/3', 'Iteration: 10', 'Train loss: 0.056')\n",
      "('Epoch: 0/3', 'Iteration: 15', 'Train loss: 0.056')\n",
      "('Epoch: 0/3', 'Iteration: 20', 'Train loss: 0.055')\n",
      "('Epoch: 0/3', 'Iteration: 25', 'Train loss: 0.055')\n",
      "Val acc: 0.938\n",
      "('Epoch: 1/3', 'Iteration: 30', 'Train loss: 0.054')\n",
      "('Epoch: 1/3', 'Iteration: 35', 'Train loss: 0.055')\n",
      "('Epoch: 1/3', 'Iteration: 40', 'Train loss: 0.054')\n",
      "('Epoch: 1/3', 'Iteration: 45', 'Train loss: 0.051')\n",
      "('Epoch: 1/3', 'Iteration: 50', 'Train loss: 0.054')\n",
      "Val acc: 0.937\n",
      "('Epoch: 2/3', 'Iteration: 55', 'Train loss: 0.052')\n",
      "('Epoch: 2/3', 'Iteration: 60', 'Train loss: 0.053')\n",
      "('Epoch: 2/3', 'Iteration: 65', 'Train loss: 0.052')\n",
      "('Epoch: 2/3', 'Iteration: 70', 'Train loss: 0.050')\n",
      "('Epoch: 2/3', 'Iteration: 75', 'Train loss: 0.046')\n",
      "Val acc: 0.930\n",
      "('Epoch: 2/3', 'Iteration: 80', 'Train loss: 0.048')\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
    "            feed = {input_data: x,\n",
    "                    labels_: y,\n",
    "                    keep_prob: 1.0,\n",
    "                    initial_state: state}\n",
    "            loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)\n",
    "            \n",
    "            if iteration%5==0:\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {:.3f}\".format(loss))\n",
    "\n",
    "            if iteration%25==0:\n",
    "                val_acc = []\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                for x, y in get_batches(val_x, val_y, batch_size):\n",
    "                    feed = {input_data: x,\n",
    "                            labels_: y,\n",
    "                            keep_prob: 1,\n",
    "                            initial_state: val_state}\n",
    "                    batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                    val_acc.append(batch_acc)\n",
    "                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "            iteration +=1\n",
    "    saver.save(sess, \"checkpoints/mbti.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/mbti.ckpt\n",
      "Test accuracy: 0.933\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    for ii, (x, y) in enumerate(get_batches(test_x, test_y, batch_size), 1):\n",
    "        feed = {input_data: x,\n",
    "                labels_: y,\n",
    "                keep_prob: 1,\n",
    "                initial_state: test_state}\n",
    "        batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test accuracy: {:.3f}\".format(np.mean(test_acc)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
